{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pm4py as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#------------------------------------------------------------------\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "#------------------------------------------------------------------\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.objects.conversion.log import converter as log_converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_folder = \"../orig_logs\"\n",
    "output_data_folder = \"../input_files\"\n",
    "in_filename_xes = \"sepsis_cases.xes\"\n",
    "in_filename_csv = \"sepsis_cases.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant = xes_importer.Variants.LINE_BY_LINE\n",
    "parameters = {variant.value.Parameters.TIMESTAMP_SORT: True}\n",
    "log = xes_importer.apply(os.path.join(input_data_folder, in_filename_xes),\n",
    "                         variant=variant, parameters=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = log_converter.apply(log, variant=log_converter.Variants.TO_DATA_FRAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.rename(columns={'case:concept:name': 'Case ID', 'concept:name': 'Activity', \n",
    "                          'time:timestamp': 'Complete Timestamp', 'org:group' : 'user'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_csv(os.path.join(input_data_folder, in_filename_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_id_col = \"Case ID\"\n",
    "activity_col = \"Activity\"\n",
    "timestamp_col = \"Complete Timestamp\"\n",
    "label_col = \"label\"\n",
    "pos_label = \"deviant\"\n",
    "neg_label = \"regular\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_freq_threshold = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_cat_cols = [\"Activity\", 'user'] # i.e. event attributes\n",
    "static_cat_cols = ['Diagnose', 'DiagnosticArtAstrup', 'DiagnosticBlood', 'DiagnosticECG',\n",
    "       'DiagnosticIC', 'DiagnosticLacticAcid', 'DiagnosticLiquor',\n",
    "       'DiagnosticOther', 'DiagnosticSputum', 'DiagnosticUrinaryCulture',\n",
    "       'DiagnosticUrinarySediment', 'DiagnosticXthorax', 'DisfuncOrg',\n",
    "       'Hypotensie', 'Hypoxie', 'InfectionSuspected', 'Infusion', 'Oligurie',\n",
    "       'SIRSCritHeartRate', 'SIRSCritLeucos', 'SIRSCritTachypnea',\n",
    "       'SIRSCritTemperature', 'SIRSCriteria2OrMore'] # i.e. case attributes that are known from the start\n",
    "dynamic_num_cols = ['CRP', 'LacticAcid', 'Leucocytes']\n",
    "static_num_cols = ['Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_cols = static_cat_cols + static_num_cols + [case_id_col]\n",
    "dynamic_cols = dynamic_cat_cols + dynamic_num_cols + [timestamp_col]\n",
    "cat_cols = dynamic_cat_cols + static_cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_timestamp_features(group):\n",
    "    \n",
    "    group = group.sort_values(timestamp_col, ascending=False, kind='mergesort')\n",
    "    \n",
    "    tmp = group[timestamp_col] - group[timestamp_col].shift(-1)\n",
    "    #tmp = tmp.fillna(0)\n",
    "    #group[\"timesincelastevent\"] = tmp.apply(lambda x: float(x / np.timedelta64(1, 'm'))) # m is for minutes\n",
    "    group[\"timesincelastevent\"] = tmp.apply(lambda x: float(x / pd.Timedelta(1, 'm'))) # m is for minutes\n",
    "\n",
    "    tmp = group[timestamp_col] - group[timestamp_col].iloc[-1]\n",
    "    #tmp = tmp.fillna(0)\n",
    "    #group[\"timesincecasestart\"] = tmp.apply(lambda x: float(x / np.timedelta64(1, 'm'))) # m is for minutes\n",
    "    group[\"timesincecasestart\"] = tmp.apply(lambda x: float(x / pd.Timedelta(1, 'm'))) # m is for minutes\n",
    "\n",
    "    group = group.sort_values(timestamp_col, ascending=True, kind='mergesort')\n",
    "    group[\"event_nr\"] = range(1, len(group) + 1)\n",
    "    \n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_before_activity(group):\n",
    "    relevant_activity_idxs = np.where(group[activity_col] == relevant_activity)[0]\n",
    "    if len(relevant_activity_idxs) > 0:\n",
    "        cut_idx = relevant_activity_idxs[0]\n",
    "        return group[:cut_idx]\n",
    "    else:\n",
    "        return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_open_cases(date):\n",
    "    return sum((dt_first_last_timestamps[\"start_time\"] <= date) & (dt_first_last_timestamps[\"end_time\"] > date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_activity_exists(group, activity):\n",
    "    relevant_activity_idxs = np.where(group[activity_col] == activity)[0]\n",
    "    if len(relevant_activity_idxs) > 0:\n",
    "        idx = relevant_activity_idxs[0]\n",
    "        group[label_col] = pos_label\n",
    "        return group[:idx]\n",
    "    else:\n",
    "        group[label_col] = neg_label\n",
    "        return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_activity_exists_and_time_less_than(group, activity):\n",
    "    relevant_activity_idxs = np.where(group[activity_col] == activity)[0]\n",
    "    if len(relevant_activity_idxs) > 0:\n",
    "        idx = relevant_activity_idxs[0]\n",
    "        if group[\"timesincelastevent\"].iloc[idx] <= 28 * 1440: # return in less than 28 days\n",
    "            group[label_col] = pos_label\n",
    "            return group[:idx]\n",
    "        else:\n",
    "            group[label_col] = neg_label\n",
    "            return group[:idx]\n",
    "    else:\n",
    "        group[label_col] = neg_label\n",
    "        return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_any_of_activities_exist(group, activities):\n",
    "    if np.sum(group[activity_col].isin(activities)) > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(input_data_folder, in_filename_csv), sep=\",\")\n",
    "data.rename(columns={\"Unnamed: 0\" : \"event_nr\"}, inplace=True)\n",
    "data[case_id_col] = data[case_id_col].fillna(\"missing_caseid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove incomplete cases\n",
    "tmp = data.groupby(case_id_col).apply(check_if_any_of_activities_exist, activities=[\"Release A\", \"Release B\", \"Release C\", \"Release D\", \"Release E\"])\n",
    "incomplete_cases = tmp.index[tmp==False]\n",
    "data = data[~data[case_id_col].isin(incomplete_cases)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns = [static_cols + dynamic_cols]\n",
    "#data = data.reindex(columns=columns)\n",
    "data = data[static_cols + dynamic_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add features extracted from timestamp\n",
    "data[timestamp_col] = pd.to_datetime(data[timestamp_col], utc=True)\n",
    "data[\"timesincemidnight\"] = data[timestamp_col].dt.hour * 60 + data[timestamp_col].dt.minute\n",
    "data[\"month\"] = data[timestamp_col].dt.month\n",
    "data[\"weekday\"] = data[timestamp_col].dt.weekday\n",
    "data[\"hour\"] = data[timestamp_col].dt.hour\n",
    "data = data.groupby(case_id_col).apply(extract_timestamp_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add inter-case features\n",
    "data = data.sort_values([timestamp_col], ascending=True, kind='mergesort')\n",
    "dt_first_last_timestamps = data.groupby(case_id_col)[timestamp_col].agg([min, max])\n",
    "dt_first_last_timestamps.columns = [\"start_time\", \"end_time\"]\n",
    "data[\"open_cases\"] = data[timestamp_col].apply(get_open_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values\n",
    "grouped = data.sort_values(timestamp_col, ascending=True, kind='mergesort').groupby(case_id_col)\n",
    "for col in static_cols + dynamic_cols:\n",
    "    data[col] = grouped[col].transform(lambda grp: grp.fillna(method='ffill'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[cat_cols] = data[cat_cols].fillna('missing')\n",
    "data = data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    counts = data[col].value_counts()\n",
    "    mask = data[col].isin(counts[counts >= category_freq_threshold].index)\n",
    "    data.loc[~mask, col] = \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first labeling\n",
    "dt_labeledbf = data.sort_values(timestamp_col, ascending=True, kind=\"mergesort\").groupby(case_id_col).apply(check_if_activity_exists_and_time_less_than, activity=\"Return ER\")\n",
    "dt_labeledbf.to_csv(os.path.join(output_data_folder, \"sepsis_cases_before.csv\"), sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13422, 39)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the Columns which is not necessary\n",
    "\n",
    "essencial_col1 = ['Case ID', 'Complete Timestamp', 'user']\n",
    "\n",
    "#additional_cols = ['event_nr', 'month', 'weekday', 'hour', 'timesincemidnight', 'timesincelastevent', 'timesincecasestart', 'open_cases']\n",
    "additional_cols = ['timesincemidnight', 'month', 'weekday', 'hour']\n",
    "columns_to_drop = essencial_col1 + additional_cols\n",
    "#print(\"Dropped Columns : \", columns_to_drop)\n",
    "#data = data.drop(columns_to_drop, axis=1)\n",
    "#Backup of data\n",
    "#dataf = data\n",
    "\n",
    "dataf = data.drop(columns_to_drop, axis=1)\n",
    "\n",
    "feature_cols = static_cat_cols + static_num_cols + [\"Activity\"]\n",
    "\n",
    "#dataf = data[feature_cols]\n",
    "\n",
    "#Get the Numeric Columns so that Columns which is Catorical can be filtered out\n",
    "\n",
    "data_list = dataf.columns\n",
    "\n",
    "data_num_cols = dataf._get_numeric_data().columns\n",
    "\n",
    "data_cat_cols = list(set(data_list) - set(data_num_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DisfuncOrg',\n",
       " 'Oligurie',\n",
       " 'InfectionSuspected',\n",
       " 'DiagnosticIC',\n",
       " 'SIRSCritTachypnea',\n",
       " 'Hypoxie',\n",
       " 'DiagnosticOther',\n",
       " 'SIRSCriteria2OrMore',\n",
       " 'Activity',\n",
       " 'DiagnosticLacticAcid',\n",
       " 'DiagnosticUrinarySediment',\n",
       " 'SIRSCritHeartRate',\n",
       " 'DiagnosticBlood',\n",
       " 'Hypotensie',\n",
       " 'DiagnosticXthorax',\n",
       " 'DiagnosticSputum',\n",
       " 'SIRSCritTemperature',\n",
       " 'DiagnosticLiquor',\n",
       " 'DiagnosticArtAstrup',\n",
       " 'Diagnose',\n",
       " 'DiagnosticUrinaryCulture',\n",
       " 'Infusion',\n",
       " 'DiagnosticECG',\n",
       " 'SIRSCritLeucos']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataf = dataf[~dataf.Diagnose.isin(['missing']) & ~dataf.DiagnosticArtAstrup.isin(['missing'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for label encoding\n",
    "data_cat_dict = {}\n",
    "for i in range(len(data_cat_cols)):\n",
    "\n",
    "    _label = data_cat_cols[i]\n",
    "    temp_list = dataf[[_label]].values.tolist()\n",
    "    subsec_set = {(x[0]) for x in temp_list}\n",
    "    \n",
    "    subsec_set = sorted(list(subsec_set))\n",
    "    _index = dict()\n",
    "\n",
    "    for ix, _ in enumerate(subsec_set):\n",
    "\n",
    "        _index[subsec_set[ix]] = ix + 1\n",
    "    \n",
    "    _idx = lambda x: _index[x[_label]]\n",
    "    \n",
    "    #Actual Vlaues in the dictionary\n",
    "    data_cat_dict[_label] = _index\n",
    "\n",
    "    dataf[_label] = dataf.apply(_idx, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels are the values we want to predict\n",
    "labels_activity = np.array(dataf['Activity'])\n",
    "\n",
    "# Remove the labels from the features\n",
    "# axis 1 refers to the columns\n",
    "dataf = dataf.drop('Activity', axis = 1)\n",
    "# Saving feature names for later use\n",
    "data_list = list(dataf.columns)\n",
    "# Convert to numpy array\n",
    "dataf = np.array(dataf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(dataf, labels_activity, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=0)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a random forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1)\n",
    "clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the name and gini importance of each feature\n",
    "feature_list = []\n",
    "for feature in zip(data_list, clf.feature_importances_):\n",
    "    feature_list.append(feature)\n",
    "    \n",
    "#feature to dictionary\n",
    "result = dict(feature_list)\n",
    "#sorting features\n",
    "result = dict(sorted(result.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'event_nr': 0.19652179878610154,\n",
       " 'timesincecasestart': 0.16603291762528904,\n",
       " 'timesincelastevent': 0.15659215822247052,\n",
       " 'Leucocytes': 0.09520168092038814,\n",
       " 'CRP': 0.0901182982089405,\n",
       " 'LacticAcid': 0.0682098557399746,\n",
       " 'open_cases': 0.05878026783010485,\n",
       " 'Diagnose': 0.040121883532746566,\n",
       " 'Age': 0.034871027545316235,\n",
       " 'DiagnosticArtAstrup': 0.009793070261011097,\n",
       " 'SIRSCritTachypnea': 0.009155887479117674,\n",
       " 'DiagnosticUrinaryCulture': 0.00898765509923254,\n",
       " 'DiagnosticUrinarySediment': 0.008523268512213342,\n",
       " 'SIRSCritTemperature': 0.005794842186913388,\n",
       " 'SIRSCritHeartRate': 0.005392555242714382,\n",
       " 'DiagnosticECG': 0.005208364028590732,\n",
       " 'Infusion': 0.004236784892930051,\n",
       " 'DiagnosticXthorax': 0.004229179674688459,\n",
       " 'SIRSCritLeucos': 0.004104135462359808,\n",
       " 'Hypotensie': 0.003923020943745561,\n",
       " 'DiagnosticLacticAcid': 0.003666933299168466,\n",
       " 'DisfuncOrg': 0.003501430152508376,\n",
       " 'DiagnosticBlood': 0.003219469071041575,\n",
       " 'Oligurie': 0.002607188262054725,\n",
       " 'DiagnosticSputum': 0.0024995292495827996,\n",
       " 'Hypoxie': 0.0018825341800408543,\n",
       " 'DiagnosticIC': 0.0018053059313772838,\n",
       " 'InfectionSuspected': 0.0017322200349334741,\n",
       " 'SIRSCriteria2OrMore': 0.0016434750252652926,\n",
       " 'DiagnosticOther': 0.0008548770088088926,\n",
       " 'DiagnosticLiquor': 0.0007883855903693554}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=RandomForestClassifier(n_estimators=1000, n_jobs=-1,\n",
       "                                                 random_state=0),\n",
       "                threshold=0.15)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a selector object that will use the random forest classifier to identify\n",
    "# features that have an importance of more than 0.15\n",
    "sfm = SelectFromModel(clf, threshold=0.15)\n",
    "\n",
    "# Train the selector\n",
    "sfm.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnose\n",
      "Age\n"
     ]
    }
   ],
   "source": [
    "# Print the names of the most important features\n",
    "data_list_important = []\n",
    "for feature_list_index in sfm.get_support(indices=True):\n",
    "    print(data_list[feature_list_index])\n",
    "    data_list_important.append(data_list[feature_list_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data to create a new dataset containing only the most important features\n",
    "# Note: We have to apply the transform to both the training X and test X data.\n",
    "X_important_train = sfm.transform(train_features)\n",
    "X_important_test = sfm.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=0)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new random forest classifier for the most important features\n",
    "clf_important = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1)\n",
    "\n",
    "# Train the new classifier on the new dataset containing the most important features\n",
    "clf_important.fit(X_important_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1441061199879409"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply The Full Featured Classifier To The Test Data\n",
    "y_pred = clf.predict(test_features)\n",
    "\n",
    "# View The Accuracy Of Our Full Feature (4 Features) Model\n",
    "accuracy_score(test_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1863129333735303"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply The Full Featured Classifier To The Test Data\n",
    "y_important_pred = clf_important.predict(X_important_test)\n",
    "\n",
    "# View The Accuracy Of Our Limited Feature Model\n",
    "accuracy_score(test_labels, y_important_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "essencial_cols = ['Case ID', 'Activity', 'Complete Timestamp', 'user']\n",
    "\n",
    "cols = essencial_cols + ['label'] + data_list_important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first labeling\n",
    "dt_labeled1 = data.sort_values(timestamp_col, ascending=True, kind=\"mergesort\").groupby(case_id_col).apply(check_if_activity_exists_and_time_less_than, activity=\"Return ER\")\n",
    "dt_labeled1[cols].to_csv(os.path.join(output_data_folder, \"sepsis_cases_1.csv\"), sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second labeling\n",
    "dt_labeled2 = data.sort_values(timestamp_col, ascending=True, kind=\"mergesort\").groupby(case_id_col).apply(check_if_activity_exists, activity=\"Admission IC\")\n",
    "dt_labeled2[cols].to_csv(os.path.join(output_data_folder, \"sepsis_cases_2.csv\"), sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fourth labeling\n",
    "dt_labeled3 = data.sort_values(timestamp_col, ascending=True, kind=\"mergesort\").groupby(case_id_col).apply(check_if_activity_exists, activity=\"Release A\")\n",
    "dt_labeled3[cols].to_csv(os.path.join(output_data_folder, \"sepsis_cases_4.csv\"), sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[essencial_cols + data_list_important].to_csv(os.path.join(output_data_folder, \"sepsis_cases.csv\"), sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdList = [dt_labeled1, dt_labeled2, dt_labeled3]  # List of your dataframes\n",
    "dt_labeled = pd.concat(pdList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36507, 40)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_labeled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13422, 39)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_labeled = dt_labeled.drop_duplicates(subset=['Case ID', 'Activity', 'Complete Timestamp', 'user']+ data_list_important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dt_labeled = dt_labeled.drop_duplicates(subset=essencial_cols+static_cat_cols+dynamic_num_cols+static_num_cols+['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13373, 40)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_labeled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_labeled[cols].to_csv(os.path.join(output_data_folder, \"sepsis_cases.csv\"), sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Product', 'Price'], ['Tablet', 250], ['iPhone', 800], ['Laptop', 1200], ['Monitor', 300]]\n"
     ]
    }
   ],
   "source": [
    "products = {'Product': ['Tablet','iPhone','Laptop','Monitor'],\n",
    "            'Price': [250,800,1200,300]\n",
    "            }\n",
    "\n",
    "df = pd.DataFrame(products, columns= ['Product', 'Price'])\n",
    "\n",
    "products_list = [df.columns.values.tolist()] + df.values.tolist()\n",
    "print (products_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product |Price          \n",
      "Tablet  |250            \n",
      "iPhone  |800            \n",
      "Laptop  |1200           \n",
      "Monitor |300            \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "products = {'Product': ['Tablet','iPhone','Laptop','Monitor'],\n",
    "            'Price': [250,800,1200,300]\n",
    "            }\n",
    "\n",
    "df = pd.DataFrame(products, columns= ['Product', 'Price'])\n",
    "\n",
    "products_list = [df.columns.values.tolist()] + df.values.tolist()\n",
    "f = '{:<8}|{:<15}' # formatting\n",
    "\n",
    "for i in products_list:\n",
    "    print(f.format(*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tablet</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iPhone</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Laptop</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Monitor</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Product  Price\n",
       "0   Tablet    250\n",
       "1   iPhone    800\n",
       "2   Laptop   1200\n",
       "3  Monitor    300"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tablet</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iPhone</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Laptop</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Monitor</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Predicted  Price\n",
       "0    Tablet    250\n",
       "1    iPhone    800\n",
       "2    Laptop   1200\n",
       "3   Monitor    300"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(columns={\"Product\" : 'Predicted'}, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tablet</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iPhone</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Laptop</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Monitor</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Product  Price\n",
       "0   Tablet    250\n",
       "1   iPhone    800\n",
       "2   Laptop   1200\n",
       "3  Monitor    300"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
